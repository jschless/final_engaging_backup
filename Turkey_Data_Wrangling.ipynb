{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import datetime \n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_DIR = '/pool001/jschless/kiran-data/kiran-data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull in Friends Data\n",
    "Store it in a dictionary of the form {username: set(friends of username)} \n",
    "\n",
    "Takes 40 seconds to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_friends(filename):\n",
    "    return (\n",
    "        pd.read_csv(os.path.join(DATA_DIR, filename),  # loading data\n",
    "                    sep='\\t', names=['user', 'friend'])\n",
    "        .groupby('user') # group by the user \n",
    "        .apply(lambda x: set(x.friend)) # get set of all friends\n",
    "        .to_dict() # convert it to a python dictionary\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 8s, sys: 18.4 s, total: 4min 27s\n",
      "Wall time: 4min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "friends_dict = {**get_friends('FRIENDS.txt'), **get_friends('FRIENDS1.txt')}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Convert Tweet Data into Pipeline Format\n",
    "For each hashtag, we need to find who saw a tweet from a friend using the hashtag before participating\n",
    "\n",
    "Also, create mapping of user ids to user names\n",
    "\n",
    "Takes 1.5 minutes to run\n",
    "\n",
    "__Errors: 540 of the tweets do not properly parse as JSONs. Not sure what happened, but different issues each time. Was such a small number I just ignored it.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# errors 540\n",
      "CPU times: user 1min 30s, sys: 3.74 s, total: 1min 33s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "# only store the following tweet columns, saves time and memory\n",
    "columns_needed = set(['author_id', 'screen_name', 'created_at', \n",
    "                      'date', 'id', 'text', 'trend', 'trend_date',\n",
    "                      'favorite_count'])\n",
    "\n",
    "tweet_dict = {}\n",
    "name_to_id = {}\n",
    "id_to_name = {}\n",
    "\n",
    "with open(os.path.join(DATA_DIR, 'TWEETINFO.txt'), 'r') as f:\n",
    "    errors = 0\n",
    "    for i, line in enumerate(f):\n",
    "        try:\n",
    "            tweet = json.loads(line)\n",
    "            \n",
    "            id_to_name[tweet['user']['id']] = tweet['user']['screen_name']\n",
    "            name_to_id[tweet['user']['screen_name']] = tweet['user']['id']\n",
    "            \n",
    "            filtered_tweet = {k:v for k,v in tweet.items() if k in columns_needed}\n",
    "            \n",
    "            if 'followers_count' in tweet.get('user', {}):\n",
    "                filtered_tweet['n_followers'] = tweet['user']['followers_count']\n",
    "            if 'friends_count' in tweet.get('user', {}):\n",
    "                filtered_tweet['n_friends'] = tweet['user']['friends_count']\n",
    "            if 'statuses_count' in tweet.get('user', {}):\n",
    "                filtered_tweet['n_statuses'] = tweet['user']['statuses_count']\n",
    "            \n",
    "            tweet_dict[tweet['id']] = filtered_tweet\n",
    "        except Exception as e:\n",
    "            errors += 1\n",
    "\n",
    "print(\"# errors\", errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load ht mapping file\n",
    "ht_mapping = pd.read_csv(\n",
    "    os.path.join(DATA_DIR, 'hashtag_mapping.txt'), \n",
    "    sep='\\t', \n",
    "    header=None, \n",
    "    index_col=1).to_dict()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing Dictionary of Hashtag Tweets\n",
    "\n",
    "__Errors: 21k of the tweets had issues. The tweet_id did not exist in the TWEETINFO.txt file. Many of them appear to be private accounts__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 418/418 [00:39<00:00, 10.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# errors 20961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create a dictionary of the form {hashtag: list[tweets using hashtag]}\n",
    "campaigns = {}\n",
    "errors = 0\n",
    "for ht_id in tqdm(os.listdir(os.path.join(DATA_DIR, 'hashtag_data'))):\n",
    "    ht = ht_mapping[int(ht_id)]\n",
    "    tweets = []\n",
    "    with open(os.path.join(DATA_DIR, 'hashtag_data', ht_id)) as f:\n",
    "        for link in f:\n",
    "            tokens = link.split('/')\n",
    "            tweet_id = int(tokens[-1])\n",
    "\n",
    "            try: \n",
    "                tweet = tweet_dict[tweet_id]\n",
    "                tweet['author'] = tokens[3]\n",
    "                tweet['author_id'] = name_to_id.get(tokens[3], -1)\n",
    "                tweet['trend'] = ht\n",
    "                if isinstance(tweet['created_at'], str):\n",
    "                    # if the created at is not a date, convert it\n",
    "                    tweet['created_at'] = datetime.datetime.strptime(tweet['created_at'], \n",
    "                                                                     '%a %b %d %H:%M:%S +0000 %Y')\n",
    "                tweets.append(tweet)\n",
    "            except Exception as e:\n",
    "#                 print(e, link)\n",
    "                errors += 1\n",
    "    campaigns[ht] = tweets    \n",
    "print('# errors', errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Integration with Original Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TURKEY_DIR = '/pool001/jschless/turkish_astroturfing'\n",
    "\n",
    "df = pd.read_csv(os.path.join(TURKEY_DIR, 'trend_tweets.csv'),\n",
    "                parse_dates=['date', 'trend_date', 'created_at'])\n",
    "\n",
    "old_campaigns = df.groupby(\"trend\").apply(lambda x: x.to_dict(orient=\"records\")).to_dict()\n",
    "del df \n",
    "\n",
    "# fill in missing author names from Tugrulcan's data\n",
    "for ht, tweets in old_campaigns.items():\n",
    "    for tweet in tweets:\n",
    "        tweet['author'] = id_to_name.get(tweet['author_id'], 'missing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge original data with the new data\n",
    "for ht in campaigns.keys():\n",
    "    campaigns[ht] += old_campaigns.get(ht, [])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exposure Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR, 'followers_dict.pkl'), 'rb') as f:\n",
    "    followers_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 418/418 [01:45<00:00,  3.97it/s]\n"
     ]
    }
   ],
   "source": [
    "for ht, tweets in tqdm(campaigns.items()):\n",
    "    tweeted = set()\n",
    "#    exposed = set()\n",
    "    sorted_tweets = sorted(tweets, key=lambda x: x['created_at'])\n",
    "    for tweet in sorted_tweets:\n",
    "        # take the intersection of the set of friends and the set of people who have already used the hashtag\n",
    "        # if this is non empty, they are unexposed\n",
    " #       n_prev_exposed = len(exposed)\n",
    "        # update the set of all people exposed\n",
    "  #      exposed = exposed.union(followers_dict.get(tweet['author'], set()))\n",
    "\n",
    "        # check if any of friends are in the tweeted set\n",
    "        tweet['exposed'] = len(friends_dict.get(tweet['author'], set()).intersection(tweeted)) != 0\n",
    "   #     tweet['n_newly_exposed'] = len(exposed) - n_prev_exposed\n",
    "        tweeted.add(tweet['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpointing\n",
    "with open(os.path.join(DATA_DIR, 'campaigns_3.pkl'), 'wb') as f:\n",
    "    pickle.dump(campaigns, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR, 'campaigns_3.pkl'), 'rb') as f:\n",
    "    campaigns = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 24s, sys: 17 s, total: 3min 41s\n",
      "Wall time: 3min 41s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df = pd.DataFrame()\n",
    "for ht, tweets in campaigns.items():\n",
    "    df = df.append(pd.DataFrame.from_records(tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>n_followers</th>\n",
       "      <th>n_friends</th>\n",
       "      <th>n_statuses</th>\n",
       "      <th>author</th>\n",
       "      <th>author_id</th>\n",
       "      <th>trend</th>\n",
       "      <th>exposed</th>\n",
       "      <th>n_newly_exposed</th>\n",
       "      <th>date</th>\n",
       "      <th>trend_date</th>\n",
       "      <th>tweet_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-05-02 14:05:16</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1256585550796148738</td>\n",
       "      <td>#MilliGazeteOkuyorum #SesimizBir #Cumartesi #D...</td>\n",
       "      <td>1767.0</td>\n",
       "      <td>2161.0</td>\n",
       "      <td>142612.0</td>\n",
       "      <td>GunesliGuzel</td>\n",
       "      <td>293656352</td>\n",
       "      <td>#ÜniversiteliİşçilereAdalet</td>\n",
       "      <td>False</td>\n",
       "      <td>276.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-11-18 17:38:31</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1196482833696645120</td>\n",
       "      <td>Kamuda çalışan üniversiteli işçiler memur stat...</td>\n",
       "      <td>809.0</td>\n",
       "      <td>2573.0</td>\n",
       "      <td>10232.0</td>\n",
       "      <td>yaprakergen</td>\n",
       "      <td>325766266</td>\n",
       "      <td>#ÜniversiteliİşçilereAdalet</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-11-11 17:02:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1193937167007068160</td>\n",
       "      <td>@MemurSenKonf Üniversite mezunu 4D'li işçiler ...</td>\n",
       "      <td>1245.0</td>\n",
       "      <td>1310.0</td>\n",
       "      <td>1624.0</td>\n",
       "      <td>Erdemakkusss</td>\n",
       "      <td>965200054352076800</td>\n",
       "      <td>#ÜniversiteliİşçilereAdalet</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-11-11 15:21:30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1193911634013630466</td>\n",
       "      <td>@_aliyalcin_ Üniversite mezunu 4D'li işçiler o...</td>\n",
       "      <td>1245.0</td>\n",
       "      <td>1310.0</td>\n",
       "      <td>1624.0</td>\n",
       "      <td>Erdemakkusss</td>\n",
       "      <td>965200054352076800</td>\n",
       "      <td>#ÜniversiteliİşçilereAdalet</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-11-11 15:21:17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1193911580230062080</td>\n",
       "      <td>@SabahMemurlar Üniversite mezunu 4D'li işçiler...</td>\n",
       "      <td>1245.0</td>\n",
       "      <td>1310.0</td>\n",
       "      <td>1624.0</td>\n",
       "      <td>Erdemakkusss</td>\n",
       "      <td>965200054352076800</td>\n",
       "      <td>#ÜniversiteliİşçilereAdalet</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           created_at  favorite_count                   id  \\\n",
       "0 2020-05-02 14:05:16             1.0  1256585550796148738   \n",
       "1 2019-11-18 17:38:31            10.0  1196482833696645120   \n",
       "2 2019-11-11 17:02:57             NaN  1193937167007068160   \n",
       "3 2019-11-11 15:21:30             NaN  1193911634013630466   \n",
       "4 2019-11-11 15:21:17             NaN  1193911580230062080   \n",
       "\n",
       "                                                text  n_followers  n_friends  \\\n",
       "0  #MilliGazeteOkuyorum #SesimizBir #Cumartesi #D...       1767.0     2161.0   \n",
       "1  Kamuda çalışan üniversiteli işçiler memur stat...        809.0     2573.0   \n",
       "2  @MemurSenKonf Üniversite mezunu 4D'li işçiler ...       1245.0     1310.0   \n",
       "3  @_aliyalcin_ Üniversite mezunu 4D'li işçiler o...       1245.0     1310.0   \n",
       "4  @SabahMemurlar Üniversite mezunu 4D'li işçiler...       1245.0     1310.0   \n",
       "\n",
       "   n_statuses        author           author_id                        trend  \\\n",
       "0    142612.0  GunesliGuzel           293656352  #ÜniversiteliİşçilereAdalet   \n",
       "1     10232.0   yaprakergen           325766266  #ÜniversiteliİşçilereAdalet   \n",
       "2      1624.0  Erdemakkusss  965200054352076800  #ÜniversiteliİşçilereAdalet   \n",
       "3      1624.0  Erdemakkusss  965200054352076800  #ÜniversiteliİşçilereAdalet   \n",
       "4      1624.0  Erdemakkusss  965200054352076800  #ÜniversiteliİşçilereAdalet   \n",
       "\n",
       "   exposed  n_newly_exposed date trend_date tweet_type  \n",
       "0    False            276.0  NaT        NaT        NaN  \n",
       "1    False              0.0  NaT        NaT        NaN  \n",
       "2     True              0.0  NaT        NaT        NaN  \n",
       "3     True              0.0  NaT        NaT        NaN  \n",
       "4     True              0.0  NaT        NaT        NaN  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trending Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[48;5;10;38;5;21mfollower_info\u001b[0m/             \u001b[38;5;34mtrend_tweets.csv\u001b[0m*\r\n",
      "\u001b[38;5;27m__MACOSX\u001b[0m/                  \u001b[38;5;34mtrend_tweets.csv.zip\u001b[0m*\r\n",
      "\u001b[38;5;34mtrend_analysis.csv\u001b[0m*        trend_tweets_w_error.csv\r\n",
      "\u001b[38;5;34mtrend_analysis_top10.csv\u001b[0m*  \u001b[38;5;34mworld_trend_analysis.csv\u001b[0m*\r\n",
      "\u001b[38;5;34mtrend_tweets_copy.csv\u001b[0m*     \u001b[38;5;34mworld_trend_analysis_top10.csv\u001b[0m*\r\n"
     ]
    }
   ],
   "source": [
    "ls /pool001/jschless/turkish_astroturfing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tr_start</th>\n",
       "      <th>tr_end</th>\n",
       "      <th>vol</th>\n",
       "      <th>max_rank</th>\n",
       "      <th>lifetime</th>\n",
       "      <th>attack</th>\n",
       "      <th>trend</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-07-14 08:43:19</td>\n",
       "      <td>2019-07-14 10:38:28</td>\n",
       "      <td>69190</td>\n",
       "      <td>15</td>\n",
       "      <td>0 days 01:55:09.000000000</td>\n",
       "      <td>False</td>\n",
       "      <td>\"DEH\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-07-13 20:06:51</td>\n",
       "      <td>2019-07-13 20:31:51</td>\n",
       "      <td>70867</td>\n",
       "      <td>13</td>\n",
       "      <td>0 days 00:25:00.000000000</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Deh\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-07-13 22:06:46</td>\n",
       "      <td>2019-07-13 22:11:46</td>\n",
       "      <td>70858</td>\n",
       "      <td>23</td>\n",
       "      <td>0 days 00:05:00.000000000</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Deh\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-07-13 23:28:42</td>\n",
       "      <td>2019-07-14 00:03:43</td>\n",
       "      <td>71657</td>\n",
       "      <td>24</td>\n",
       "      <td>0 days 00:35:01.000000000</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Deh\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-07-14 08:33:21</td>\n",
       "      <td>2019-07-14 08:38:18</td>\n",
       "      <td>68323</td>\n",
       "      <td>15</td>\n",
       "      <td>0 days 00:04:57.000000000</td>\n",
       "      <td>False</td>\n",
       "      <td>\"Deh\"</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tr_start              tr_end    vol  max_rank  \\\n",
       "0 2019-07-14 08:43:19 2019-07-14 10:38:28  69190        15   \n",
       "1 2019-07-13 20:06:51 2019-07-13 20:31:51  70867        13   \n",
       "2 2019-07-13 22:06:46 2019-07-13 22:11:46  70858        23   \n",
       "3 2019-07-13 23:28:42 2019-07-14 00:03:43  71657        24   \n",
       "4 2019-07-14 08:33:21 2019-07-14 08:38:18  68323        15   \n",
       "\n",
       "                    lifetime  attack  trend  \n",
       "0  0 days 01:55:09.000000000   False  \"DEH\"  \n",
       "1  0 days 00:25:00.000000000   False  \"Deh\"  \n",
       "2  0 days 00:05:00.000000000   False  \"Deh\"  \n",
       "3  0 days 00:35:01.000000000   False  \"Deh\"  \n",
       "4  0 days 00:04:57.000000000   False  \"Deh\"  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TURKEY_DIR = '/pool001/jschless/turkish_astroturfing'\n",
    "\n",
    "# trend_file = 'trend_analysis_top10.csv' \n",
    "trend_file = 'trend_analysis.csv' \n",
    "#trend_file = 'world_trend_analysis_top10.csv'\n",
    "\n",
    "trending_info = pd.read_csv(os.path.join(TURKEY_DIR, trend_file),\n",
    "                           parse_dates=['tr_start', 'tr_end', 'lifetime', 'date'])\n",
    "\n",
    "trending_info['trend'] = trending_info.keyword\n",
    "\n",
    "trending_info = trending_info.drop(columns=['date', 'id', 'keyword'])\n",
    "\n",
    "trending_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trending_info['time_trending'] = trending_info.tr_end - trending_info.tr_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Tugrulcan's classifier for lexicon tweets\n",
    "\n",
    "import emoji\n",
    "import string\n",
    "\n",
    "def give_emoji_free_text(text):\n",
    "    allchars = [str for str in text]\n",
    "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "def lexicon_classifier(line, trend):\n",
    "    line = give_emoji_free_text(line)\n",
    "    line = line.replace(trend, '')\n",
    "    line = line.replace('  ', ' ')\n",
    "\n",
    "    line = line.strip()\n",
    "\n",
    "    if (len(line) == 0):\n",
    "        return False\n",
    "\n",
    "    if (line[0].isupper()):\n",
    "        return False\n",
    "\n",
    "    invalidChars = set(string.punctuation.replace(\"(\", \"…\").replace(\")\", \"...\").replace('.', \".\").replace('.', '.'))\n",
    "    invalidChars = invalidChars.union(set([\"%d\" % i for i in range(0,10)])) # added numbers\n",
    "\n",
    "    if any(char in invalidChars for char in line):\n",
    "        return False\n",
    "\n",
    "    tokens = line.split(' ')\n",
    "    if (len(tokens) > 10 or len(tokens) < 3):\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25min 44s, sys: 1min 3s, total: 26min 48s\n",
      "Wall time: 26min 48s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "mega_df = df.merge(trending_info, on='trend')\n",
    "mega_df[\"time_since_trending\"] = mega_df.created_at - mega_df.tr_start\n",
    "mega_df[\"time_since_trending\"] = mega_df.time_since_trending.apply(lambda x: int(x.total_seconds() / 60))\n",
    "mega_df[\"lexicon\"] = mega_df.apply(lambda x: lexicon_classifier(x.text, x.trend), axis=1)\n",
    "mega_df['follower_data'] = mega_df.author.apply(lambda x: x in friends_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save now, after a lot of the heavy lifting is done \n",
    "mega_df.to_pickle(os.path.join(DATA_DIR, 'mega_df_full_follower_final.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gt",
   "language": "python",
   "name": "gt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
